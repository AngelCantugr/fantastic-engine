#!/usr/bin/env python3
"""
Main entry point for AI-Powered Data Quality Validator

Usage:
    python run_validator.py --config configs/nyc_taxi_config.yaml
    python run_validator.py --dataset data/sample.csv --quick
"""

import click
import yaml
from pathlib import Path
from rich.console import Console
from rich.table import Table
from rich.panel import Panel
from loguru import logger
import sys

# Configure logger
logger.remove()
logger.add(sys.stderr, level="INFO")

console = Console()


@click.command()
@click.option(
    '--config',
    type=click.Path(exists=True),
    help='Path to configuration YAML file'
)
@click.option(
    '--dataset',
    type=click.Path(exists=True),
    help='Path to dataset (overrides config)'
)
@click.option(
    '--output',
    type=click.Path(),
    default='results/validation_report.md',
    help='Output path for report'
)
@click.option(
    '--quick',
    is_flag=True,
    help='Quick validation (rules only, skip AI checks)'
)
@click.option(
    '--llm-provider',
    type=click.Choice(['anthropic', 'openai', 'none']),
    default='anthropic',
    help='LLM provider for AI validation'
)
@click.option(
    '--verbose',
    is_flag=True,
    help='Verbose output'
)
def main(config, dataset, output, quick, llm_provider, verbose):
    """
    AI-Powered Data Quality Validator

    Combines rule-based validation (Great Expectations) with AI-powered
    semantic validation and anomaly explanation.
    """
    # Set log level
    if verbose:
        logger.remove()
        logger.add(sys.stderr, level="DEBUG")

    # Display banner
    console.print(Panel.fit(
        "[bold cyan]üîç AI-Powered Data Quality Validator[/bold cyan]\n"
        "[dim]Combining Rules + AI for Comprehensive Validation[/dim]",
        border_style="cyan"
    ))

    try:
        # Load configuration
        if config:
            console.print(f"[green]‚úì[/green] Loading config: {config}")
            with open(config) as f:
                cfg = yaml.safe_load(f)
        else:
            # Minimal config
            cfg = {
                'dataset': {'path': dataset or 'data/sample.csv'},
                'validation': {'rules': [], 'ai_checks': []},
                'llm': {'provider': llm_provider}
            }

        # Override dataset if provided
        if dataset:
            cfg['dataset']['path'] = dataset

        # Disable AI checks if quick mode
        if quick or llm_provider == 'none':
            cfg['validation']['ai_checks'] = []
            console.print("[yellow]‚ö†[/yellow] Quick mode: AI checks disabled")

        # Validate configuration
        dataset_path = cfg['dataset']['path']
        if not Path(dataset_path).exists():
            console.print(f"[red]‚úó[/red] Dataset not found: {dataset_path}")
            return

        console.print(f"[green]‚úì[/green] Dataset: {dataset_path}")

        # Run validation
        console.print("\n[bold]Running validation...[/bold]")
        results = run_validation(cfg, verbose=verbose)

        # Display results
        display_results(results)

        # Generate and save report
        console.print(f"\n[bold]Generating report...[/bold]")
        report = generate_report(results, cfg)

        output_path = Path(output)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        output_path.write_text(report)

        console.print(f"[green]‚úì[/green] Report saved: {output}")

        # Exit code based on results
        if results['summary']['critical_issues'] > 0:
            console.print("\n[red]‚ùå Validation FAILED (critical issues found)[/red]")
            sys.exit(1)
        elif results['summary']['warnings'] > 0:
            console.print("\n[yellow]‚ö† Validation PASSED (with warnings)[/yellow]")
            sys.exit(0)
        else:
            console.print("\n[green]‚úÖ Validation PASSED[/green]")
            sys.exit(0)

    except Exception as e:
        logger.exception("Validation failed")
        console.print(f"\n[red]‚úó Error: {e}[/red]")
        sys.exit(1)


def run_validation(config: dict, verbose: bool = False) -> dict:
    """
    Execute validation pipeline.

    Args:
        config: Configuration dictionary
        verbose: Enable verbose logging

    Returns:
        Validation results
    """
    import pandas as pd
    from src.validators.rule_based import GreatExpectationsValidator
    from src.analyzers.anomaly_explainer import AnomalyExplainer

    results = {
        'summary': {
            'total_checks': 0,
            'passed': 0,
            'failed': 0,
            'warnings': 0,
            'critical_issues': 0
        },
        'rule_validation': {},
        'ai_analysis': {},
        'dataset_info': {}
    }

    # Load dataset
    dataset_path = config['dataset']['path']
    console.print(f"  Loading dataset...")

    if dataset_path.endswith('.parquet'):
        df = pd.read_parquet(dataset_path)
    elif dataset_path.endswith('.csv'):
        df = pd.read_csv(dataset_path)
    else:
        raise ValueError(f"Unsupported format: {dataset_path}")

    results['dataset_info'] = {
        'rows': len(df),
        'columns': len(df.columns),
        'memory_mb': df.memory_usage(deep=True).sum() / 1024**2
    }

    console.print(f"  [dim]Loaded {len(df):,} rows, {len(df.columns)} columns[/dim]")

    # Rule-based validation
    if config['validation'].get('rules'):
        console.print(f"\n  [cyan]‚Üí[/cyan] Running rule-based validation...")

        validator = GreatExpectationsValidator()
        rule_results = validator.validate_from_config(df, config['validation']['rules'])

        results['rule_validation'] = rule_results
        results['summary']['total_checks'] += len(rule_results.get('results', []))
        results['summary']['passed'] += sum(
            1 for r in rule_results.get('results', []) if r['success']
        )
        results['summary']['failed'] += sum(
            1 for r in rule_results.get('results', []) if not r['success']
        )

        console.print(
            f"    [green]{results['summary']['passed']}[/green] passed, "
            f"[red]{results['summary']['failed']}[/red] failed"
        )

    # AI-powered analysis
    if config['validation'].get('ai_checks'):
        console.print(f"\n  [cyan]‚Üí[/cyan] Running AI-powered analysis...")

        llm_provider = config.get('llm', {}).get('provider', 'anthropic')
        explainer = AnomalyExplainer(llm_provider=llm_provider)

        ai_results = []
        for check in config['validation']['ai_checks']:
            try:
                console.print(f"    Analyzing: {check['name']}")

                # Run AI check (simplified for demo)
                explanation = {
                    'check_name': check['name'],
                    'status': 'completed',
                    'findings': 'AI analysis completed successfully'
                }

                ai_results.append(explanation)

            except Exception as e:
                logger.error(f"AI check failed: {e}")
                ai_results.append({
                    'check_name': check['name'],
                    'status': 'error',
                    'error': str(e)
                })

        results['ai_analysis'] = {'checks': ai_results}

    return results


def display_results(results: dict):
    """Display validation results in terminal."""

    # Summary table
    table = Table(title="Validation Summary", show_header=True)
    table.add_column("Metric", style="cyan")
    table.add_column("Value", style="yellow")

    summary = results['summary']
    dataset_info = results['dataset_info']

    table.add_row("Dataset Rows", f"{dataset_info['rows']:,}")
    table.add_row("Dataset Columns", f"{dataset_info['columns']}")
    table.add_row("Total Checks", f"{summary['total_checks']}")
    table.add_row("‚úì Passed", f"[green]{summary['passed']}[/green]")
    table.add_row("‚úó Failed", f"[red]{summary['failed']}[/red]")

    console.print("\n")
    console.print(table)


def generate_report(results: dict, config: dict) -> str:
    """Generate markdown report."""

    report = f"""# Data Quality Validation Report

**Dataset:** {config['dataset']['name']}
**Date:** {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}
**Rows:** {results['dataset_info']['rows']:,}
**Columns:** {results['dataset_info']['columns']}

---

## Summary

- Total Checks: {results['summary']['total_checks']}
- ‚úÖ Passed: {results['summary']['passed']}
- ‚ùå Failed: {results['summary']['failed']}
- ‚ö†Ô∏è Warnings: {results['summary']['warnings']}

---

## Rule-Based Validation Results

"""

    # Add rule validation details
    if results.get('rule_validation'):
        for result in results['rule_validation'].get('results', []):
            status = "‚úÖ" if result['success'] else "‚ùå"
            report += f"### {status} {result.get('expectation_type', 'Check')}\n\n"
            report += f"- Column: `{result.get('column', 'N/A')}`\n"
            report += f"- Result: {result.get('result', {})}\n\n"

    # Add AI analysis
    if results.get('ai_analysis'):
        report += "\n---\n\n## AI-Powered Analysis\n\n"
        for check in results['ai_analysis'].get('checks', []):
            report += f"### {check['check_name']}\n\n"
            report += f"- Status: {check['status']}\n"
            report += f"- Findings: {check.get('findings', 'N/A')}\n\n"

    report += "\n---\n\n*Generated by AI-Powered Data Quality Validator*\n"

    return report


if __name__ == '__main__':
    # Import pandas here for report generation
    import pandas as pd
    main()
